# 类：CNN 特征提取器
class CNNFeatureExtractor(nn.Module):
    def __init__(self, embed_size):
        super(CNNFeatureExtractor, self).__init__()
        # 加载预训练的 ResNet50 模型，并移除最后两个全连接层
        resnet = resnet50(pretrained=True)
        modules = list(resnet.children())[:-2]  # 去掉最后两个全连接层
        self.resnet = nn.Sequential(*modules)
        # 添加自适应池化层，将特征图调整为固定大小
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
        # 全连接层，将特征映射到指定的嵌入维度
        self.fc = nn.Linear(resnet.fc.in_features, embed_size)

    def forward(self, images): # images: batch,3,224,224
        # 禁用梯度计算
        #with torch.no_grad():
        # 使用 ResNet 提取特征
        features = self.resnet(images) # 64,2048,7,7
        # 自适应池化层调整特征图大小
        features = self.adaptive_pool(features) # 64,2048,1,1
        # 展平为一维向量
        features = features.view(features.size(0), -1) # 64,2048
        # 通过全连接层将特征映射到嵌入空间
        features = self.fc(features) # batch,256
        return features

# 类：Transformer 解码器
class TransformerDecoder(nn.Module):
    def __init__(self, embed_size, vocab_size, num_layers, num_heads, dropout=0.1):
        super(TransformerDecoder, self).__init__()
        # 初始化词嵌入层和位置嵌入层
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.pos_embedding = nn.Embedding(50, embed_size)
        '''
        nn.Embedding 是 PyTorch 中用于创建嵌入层（embedding layer）的类。
        嵌入层通常用于将离散的分类变量（如词汇表中的单词）映射到连续的向量空间中。
        这对于自然语言处理（NLP）任务非常有用，因为它可以将单词表示为密集的向量，从而捕捉词语的语义关系。
        词嵌入层：词汇表中有vocab_size个单词，每个单词用embed_size维向量表示
        位置嵌入层：将每个位置（最多 5000 个）映射到一个 embed_size 维的向量
        '''
        # 初始化 Transformer 编码器
        self.transformer = nn.Transformer(embed_size, num_heads, num_layers, num_layers, dropout=dropout, batch_first=True)
        '''
        使用 batch_first=True 参数时,src 和 tgt（即 features 和 captions_embed）的形状应为 (batch_size, seq_length, embed_size)
        src（features）和 tgt（captions_embed）具有相同的 batch_size 和 embed_size。
        src（features）的 seq_length 可以与 tgt（captions_embed）不同，但是需要保持形状一致。
        '''
        # 全连接层，将 Transformer 输出映射到词汇表大小
        self.fc = nn.Linear(embed_size, vocab_size)
        # 存储嵌入维度
        self.embed_size = embed_size

    # 前向传播：用于训练模型时的前向传播，它接收图像特征和对应的文本序列，输出每个时间步的预测结果
    def forward(self, features, captions):
        # features: (batch_size, feature_dim)
        # captions: (batch_size, seq_length)
        pos = torch.arange(0, captions.size(1)).unsqueeze(0).to(device)  # (1, seq_length)
        captions_embed = self.embedding(captions) + self.pos_embedding(pos)  # (batch_size, seq_length, embed_size)
        features = features.unsqueeze(1).repeat(1, captions.size(1), 1)  # (batch_size, seq_length, feature_dim)
        out = self.transformer(features, captions_embed)  # (batch_size, seq_length, embed_size)
        out = self.fc(out)  # (batch_size, seq_length, vocab_size)
        return out

    # 生成文本说明：用于在生成阶段（如测试或推理时），根据输入的图像特征逐步生成描述文本
    def generate(self, features, max_length, start_token, end_token):
        generated = torch.tensor([start_token]).unsqueeze(0).to(device)  # (1, 1)
        for _ in range(max_length):
            pos = torch.arange(0, generated.size(1)).unsqueeze(0).to(device)  # (1, seq_length)
            out = self.embedding(generated) + self.pos_embedding(pos)  # (1, seq_length, embed_size)
            features_expanded = features.unsqueeze(0).repeat(out.size(0), out.size(1), 1)  # (1, seq_length, feature_dim)
            out = self.transformer(features_expanded, out)  # (1, seq_length, embed_size)
            out = self.fc(out)  # (1, seq_length, vocab_size)
            _, next_word = torch.max(out[:, -1, :], dim=1)  # (1,)
            generated = torch.cat((generated, next_word.unsqueeze(0)), dim=1)  # (1, seq_length+1)
            if next_word.item() == end_token:
                break
        return generated.squeeze(0).tolist()

# 函数：模型训练
def train(model, train_loader, criterion, optimizer, vocab_size):
    model.train()
    total_loss = 0
    for images, captions, _ in tqdm(train_loader, desc="Training"):
        images, captions = images.to(device), captions.to(device)
        optimizer.zero_grad()
        features = model[0](images)  # (batch_size, feature_dim)
        outputs = model[1](features, captions[:, :-1])
        loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].reshape(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(train_loader)

# 函数：模型评估
def evaluate(model, val_loader, criterion, vocab_size):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for images, captions, _ in tqdm(val_loader, desc="Evaluating"):
            images, captions = images.to(device), captions.to(device)
            features = model[0](images)  # (batch_size, feature_dim)
            outputs = model[1](features, captions[:, :-1])
            loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].reshape(-1))
            total_loss += loss.item()
    return total_loss / len(val_loader)

# 函数：计算评价指标
def calculate_bleu(model, data_loader, vocab, start_token, end_token, max_length):
    predict_times = [] # 记录推理时间
    model.eval()
    smoothie = SmoothingFunction().method4
    bleu_scores = {i: [] for i in range(1, 5)}
    with torch.no_grad():
        for images, captions, _ in tqdm(data_loader, desc="Calculating BLEU"):
            images = images.to(device)
            features = model[0](images)  # (batch_size, feature_dim)
            for i in range(captions.size(0)):
                start_time = time.time()
                generated_caption = model[1].generate(features[i], max_length, start_token, end_token)
                end_time = time.time()
                predict_times.append(end_time - start_time)
                reference_caption = captions[i].tolist()
                for j in range(1, 5):
                    bleu_score = sentence_bleu([reference_caption], generated_caption, weights=[1 / j] * j, smoothing_function=smoothie)
                    bleu_scores[j].append(bleu_score)
    avg_bleu_scores = {i: np.mean(scores) for i, scores in bleu_scores.items()}
    predict_time = sum(predict_times) / 1000
    return avg_bleu_scores, predict_time

# 函数：将整数序列转为文本序列
def decode_caption(caption, vocab, reverse_vocab):
    # 过滤掉没有意义的标志
    caption = [item for item in caption if item not in [vocab['<pad>'], vocab['<start>'], vocab['<end>'], vocab['<unk>']]]
    words = [reverse_vocab[idx] for idx in caption]
    return ' '.join(words)

# 函数：显示图像
def show_imgAcap(tensor, pre_caption, real_caption, save_path=None,show=False,mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):
    # 处理caption
    pre_caption = decode_caption(pre_caption, vocab, reverse_vocab) # vocab, reverse_vocab是全局的
    real_caption = decode_caption(real_caption, vocab, reverse_vocab)
    # 处理图像
    image = tensor.clone()
    image = image.numpy().transpose((1, 2, 0))
    mean = np.array(mean)
    std = np.array(std)
    image = std * image + mean
    image = np.clip(image, 0, 1)
    # 显示图像
    plt.imshow(image)
    plt.title(pre_caption + '\n' + real_caption)
    plt.axis('off')  # Turn off axis numbers and ticks
    if save_path is not None:
        plt.savefig(os.path.join(save_path) + ".png", dpi=600)
    if show == True:
        plt.show()

# 函数：展示结果
def display_result(model, data_loader, vocab, start_token, end_token, max_length):
    model.eval()
    with torch.no_grad():
        for images, captions, _ in tqdm(data_loader, desc="Calculating BLEU"):
            images = images.to(device)
            features = model[0](images)  # (batch_size, feature_dim)
            for i in range(captions.size(0)):
                generated_caption = model[1].generate(features[i], max_length, start_token, end_token)
                reference_caption = captions[i].tolist()
                # 调用show_imgAcap函数
                os.makedirs(os.path.join(result_dir, "show"), exist_ok=True)
                show_imgAcap(tensor=images[i].detach().cpu(), pre_caption=generated_caption,
                             real_caption=reference_caption, save_path=os.path.join(result_dir, "show", str(i)))
            # 跳出循环，不画太多
            break